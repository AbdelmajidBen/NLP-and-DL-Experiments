{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eebaa996-26c4-4263-b670-cd22f1641644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('66353e4f6dd8a2c385463b7a'), 'text': 'نقلت صحيفة بوليتيكو عن مسؤول أميركي قوله إن إسرائيل أبلغت إدارة الرئيس الأميركي جو بايدن ومنظمات إغاثية أنها وضعت خطة لإجلاء السكان من مدينة رفح جنوب\\xa0قطاع غزة استعدادا لاجتياحها بريا.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b7b'), 'text': 'ونقلت الصحيفة عن المسؤول الأميركي ومصدرين مطلعين أن الخطة الإسرائيلية تتضمن نقل السكان من رفح إلى منطقة المواصي على ساحل غزة.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b7c'), 'text': 'وقد أرسل الجيش الإسرائيلي خريطة للمنطقة إلى عمال الإغاثة هذا الأسبوع، وحصلت بوليتيكو على نسخة منها.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b7d'), 'text': 'وقال المصدران المطلعان إن الجيش الإسرائيلي أخبر منظمات الإغاثة أن اجتياح رفح سيمضي قدما \"قريبا\" لكنه لم يذكر موعدا محددا.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b7e'), 'text': 'كما نقلت شبكة \"سي إن إن\" الأميركية عن مصادر قولها إن الحكومة الإسرائيلية أطلعت إدارة بايدن في الأيام الأخيرة على أحدث أفكارها بشأن إجلاء المدنيين من رفح جنوب قطاع غزة.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b7f'), 'text': 'وأضافت \"سي إن إن\" أن ما عرضته إسرائيل ليس خطة نهائية، ويركز على حركة لنقل المدنيين خارج رفح دون ذكر التفاصيل العسكرية.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b80'), 'text': 'وتعليقا على ذلك، قال متحدث باسم البيت الأبيض \"إننا لم نطلع على خطة إسرائيلية شاملة لعملية رفح، وقد عبرنا عن مخاوفنا بشأن غزو بري كبير في رفح، ونريد أن تستمر المحادثات بين واشنطن وتل أبيب بشأن ذلك\".'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b81'), 'text': 'وأضاف المتحدث أن \"هناك 1.5 مليون شخص في رفح يجب أن يحظوا بالحماية. نعمل على مدار الساعة بشأن اتفاق وقف إطلاق النار وعودة المحتجزين\".'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b82'), 'text': 'وحثت إدارة بايدن إسرائيل مرارا على تجنب غزو رفح ما لم تكن لديها خطة لحماية المدنيين الذين يحتمون هناك.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b83'), 'text': 'وحذرت منظمات دولية من اقتحام جيش الاحتلال الإسرائيلي مدينة رفح، مشيرة إلى أن ذلك قد يتسبب في \"مذبحة\" للمدنيين، وسيؤدي إلى توقف عمليات الإغاثة للقطاع.'}\n",
      "{'_id': ObjectId('66353e4f6dd8a2c385463b84'), 'text': 'تابع الجزيرة نت على:'}\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "# MongoDB connection details\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")  # Update with your MongoDB URI\n",
    "db = mongo_client[\"atelier2\"]  # Specify your database name\n",
    "collection = db[\"atelier2\"]  # Specify your collection name\n",
    "\n",
    "# Retrieve all documents from the collection\n",
    "documents = collection.find()\n",
    "\n",
    "# Display the data\n",
    "for document in documents:\n",
    "    print(document)\n",
    "\n",
    "# Close MongoDB connection\n",
    "mongo_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5acdbfe7-8d5b-4fe3-980c-45c98c5b3271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نقلت صحيفة بوليتيكو عن مسوول اميركي قوله ان اسراييل ابلغت ادارة الرييس الاميركي جو بايدن ومنظمات اغاثية انها وضعت خطة لاجلاء السكان من مدينة رفح جنوب قطاع غزة استعدادا لاجتياحها بريا\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    return text.replace('ـ', '')\n",
    "\n",
    "def process_text_from_mongodb():\n",
    "    mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = mongo_client[\"atelier2\"]\n",
    "    collection = db[\"atelier2\"]\n",
    "\n",
    "    try:\n",
    "        articleExample = collection.find_one()\n",
    "        if articleExample:\n",
    "            content = articleExample.get('text', '')\n",
    "\n",
    "            content = remove_punctuations(content)\n",
    "            content = remove_diacritics(content)\n",
    "            content = normalize_arabic(content)\n",
    "\n",
    "            contentParagraphs = [para.strip() for para in content.split('\\n') if para.strip()]\n",
    "\n",
    "            for paragraph in contentParagraphs:\n",
    "                print(paragraph)\n",
    "        else:\n",
    "            print(\"No document found in the collection.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_text_from_mongodb()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a939998d-83d7-4864-a6cb-df28f3f7503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نقلت صحيفة بوليتيكو عن مسؤول أميركي قوله إن إسرائيل أبلغت إدارة الرئيس الأميركي جو بايدن ومنظمات إغاثية أنها وضعت خطة لإجلاء السكان من مدينة رفح جنوب قطاع غزة استعدادا لاجتياحها بريا.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "if content:\n",
    "    contentParagraphs = [para.strip() for para in content.split('\\n') if para.strip()]\n",
    "    for p in contentParagraphs:\n",
    "        sentences = sent_tokenize(p)\n",
    "        for sentence in sentences:\n",
    "            print(sentence)\n",
    "        print()  \n",
    "else:\n",
    "    print(\"No text content available to tokenize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "585fc6c0-cb60-4b98-abdb-50d74b27d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      "['نقلت', 'صحيفة', 'بوليتيكو', 'عن', 'مسؤول', 'أميركي', 'قوله', 'إن', 'إسرائيل', 'أبلغت', 'إدارة', 'الرئيس', 'الأميركي', 'جو', 'بايدن', 'ومنظمات', 'إغاثية', 'أنها', 'وضعت', 'خطة', 'لإجلاء', 'السكان', 'من', 'مدينة', 'رفح', 'جنوب', 'قطاع', 'غزة', 'استعدادا', 'لاجتياحها', 'بريا', '.']\n",
      "without stopwords:\n",
      "['نقلت', 'صحيفة', 'بوليتيكو', 'مسؤول', 'أميركي', 'قوله', 'إسرائيل', 'أبلغت', 'إدارة', 'الرئيس', 'الأميركي', 'جو', 'بايدن', 'ومنظمات', 'إغاثية', 'أنها', 'وضعت', 'خطة', 'لإجلاء', 'السكان', 'مدينة', 'رفح', 'جنوب', 'قطاع', 'غزة', 'استعدادا', 'لاجتياحها', 'بريا', '.']\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('arabic'))\n",
    "contentWords = []\n",
    "\n",
    "for p in contentParagraphs:\n",
    "    words = word_tokenize(p)\n",
    "    print(\"original:\")\n",
    "    print(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    print(\"without stopwords:\")\n",
    "    print(words)\n",
    "    print(\"--------------------------------------\")\n",
    "    contentWords.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "031fbc52-9368-4785-8c66-2b84e5e07402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نقلت صحيفة بوليتيكو عن مسوول اميركي قوله ان اسراييل ابلغت ادارة الرييس الاميركي جو بايدن ومنظمات اغاثية انها وضعت خطة لاجلاء السكان من مدينة رفح جنوب قطاع غزة استعدادا لاجتياحها بريا\n",
      "\n",
      "Original Words:\n",
      "['نقلت', 'صحيفة', 'بوليتيكو', 'عن', 'مسوول', 'اميركي', 'قوله', 'ان', 'اسراييل', 'ابلغت', 'ادارة', 'الرييس', 'الاميركي', 'جو', 'بايدن', 'ومنظمات', 'اغاثية', 'انها', 'وضعت', 'خطة', 'لاجلاء', 'السكان', 'من', 'مدينة', 'رفح', 'جنوب', 'قطاع', 'غزة', 'استعدادا', 'لاجتياحها', 'بريا']\n",
      "Filtered Words (without stopwords, after stemming):\n",
      "['قلت', 'صحيف', 'وليتيكو', 'مسوول', 'اميرك', 'قول', 'ان', 'اسراييل', 'ابلغ', 'ادار', 'رييس', 'اميرك', 'جو', 'ايدن', 'منظم', 'اغاث', 'نه', 'ضع', 'خط', 'اجلاء', 'سك', 'مدين', 'رفح', 'جنوب', 'قطاع', 'غز', 'استعداد', 'اجتياح', 'ريا']\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    return text.replace('ـ', '')\n",
    "\n",
    "def process_text_from_mongodb():\n",
    "    mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = mongo_client[\"atelier2\"]\n",
    "    collection = db[\"atelier2\"]\n",
    "\n",
    "    try:\n",
    "        articleExample = collection.find_one()\n",
    "        if articleExample:\n",
    "            content = articleExample.get('text', '')\n",
    "\n",
    "            content = remove_punctuations(content)\n",
    "            content = remove_diacritics(content)\n",
    "            content = normalize_arabic(content)\n",
    "\n",
    "            contentParagraphs = [para.strip() for para in content.split('\\n') if para.strip()]\n",
    "\n",
    "            # Tokenize paragraphs into sentences and print\n",
    "            for paragraph in contentParagraphs:\n",
    "                sentences = sent_tokenize(paragraph)\n",
    "                for sentence in sentences:\n",
    "                    print(sentence)\n",
    "                print()\n",
    "\n",
    "            return contentParagraphs  # Return preprocessed content paragraphs\n",
    "        else:\n",
    "            print(\"No document found in the collection.\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nltk.download('punkt')  # Download NLTK punkt tokenizer\n",
    "\n",
    "    # Process text content from MongoDB\n",
    "    contentParagraphs = process_text_from_mongodb()\n",
    "\n",
    "    if contentParagraphs:\n",
    "        # Initialize Arabic stopwords\n",
    "        stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "        # Initialize ArabicLightStemmer\n",
    "        arabic_stemmer = ArabicLightStemmer()\n",
    "\n",
    "        # List to store stemmed words\n",
    "        contentWordsStemmed = []\n",
    "\n",
    "        # Tokenize paragraphs, remove stopwords, and perform stemming\n",
    "        for p in contentParagraphs:\n",
    "            words = word_tokenize(p)\n",
    "\n",
    "            # Remove stopwords and perform stemming\n",
    "            filtered_words = [arabic_stemmer.light_stem(w) for w in words if w not in stop_words]\n",
    "\n",
    "            # Print original words, filtered words (after stemming)\n",
    "            print(\"Original Words:\")\n",
    "            print(words)\n",
    "            print(\"Filtered Words (without stopwords, after stemming):\")\n",
    "            print(filtered_words)\n",
    "            print(\"--------------------------------------\")\n",
    "\n",
    "            # Append stemmed words to contentWordsStemmed\n",
    "            contentWordsStemmed.append(filtered_words)\n",
    "\n",
    "        # Now contentWordsStemmed contains lists of stemmed words for each paragraph\n",
    "        # You can further process or analyze contentWordsStemmed as needed\n",
    "    else:\n",
    "        print(\"No content paragraphs available for stemming.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71a4b7bd-3933-42a4-9e25-5d5d6f53d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words:\n",
      "['قلت', 'صحيف', 'وليتيكو', 'عن', 'مسوول', 'اميرك', 'قول', 'ان', 'اسراييل', 'ابلغ', 'ادار', 'رييس', 'اميرك', 'جو', 'ايدن', 'منظم', 'اغاث', 'نه', 'ضع', 'خط', 'اجلاء', 'سك', 'من', 'مدين', 'رفح', 'جنوب', 'قطاع', 'غز', 'استعداد', 'اجتياح', 'ريا.']\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import unicodedata\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = mongo_client[\"atelier2\"]\n",
    "collection = db[\"atelier2\"]\n",
    "articleExample = collection.find_one()\n",
    "if articleExample:\n",
    "    content = articleExample.get('text', '')\n",
    "    content = ''.join(char for char in unicodedata.normalize('NFD', content) if unicodedata.category(char) != 'Mn')\n",
    "    preprocessed_words = content.split()\n",
    "    arabic_stemmer = ArabicLightStemmer()\n",
    "    stemmed_words = [arabic_stemmer.light_stem(word) for word in preprocessed_words]\n",
    "    print(\"Stemmed Words:\")\n",
    "    print(stemmed_words)\n",
    "else:\n",
    "    print(\"No document found in the collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7de8ebdd-1a52-4975-97a7-508c341120ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words:\n",
      "['نقلت', 'صحيفة', 'بوليتيكو', 'مسوول', 'اميركي', 'قوله', 'ان', 'اسراييل', 'ابلغت', 'ادارة', 'الرييس', 'الاميركي', 'جو', 'بايدن', 'ومنظمات', 'اغاثية', 'انها', 'وضعت', 'خطة', 'لاجلاء', 'السكان', 'مدينة', 'رفح', 'جنوب', 'قطاع', 'غزة', 'استعدادا', 'لاجتياحها', 'بريا', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "words = word_tokenize(content)\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "filtered_words = [w for w in words if w not in stop_words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "print(\"Lemmatized Words:\")\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e5f15-8676-4d15-bf8d-f3b81d30fd86",
   "metadata": {},
   "source": [
    "## Question Number 1 \n",
    "#### Apply one hot encoding, bag of words, TF-IDF technics on the Data vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13afdb07-254d-4c83-8b4d-c69e654f941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded Representation (Sample):\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "Bag of Words Representation:\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "\n",
      "TF-IDF Representation:\n",
      "[[0.18569534 0.18569534 0.18569534 0.18569534 0.18569534 0.18569534\n",
      "  0.18569534 0.18569534 0.18569534 0.18569534 0.18569534 0.18569534\n",
      "  0.18569534 0.18569534 0.18569534 0.18569534 0.18569534 0.18569534\n",
      "  0.18569534 0.18569534 0.18569534 0.18569534 0.18569534 0.18569534\n",
      "  0.18569534 0.18569534 0.18569534 0.18569534 0.18569534]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Create a list of unique words (vocabulary)\n",
    "vocabulary = set(filtered_words)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# One-Hot Encoding\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "one_hot_encoded = encoder.fit_transform([[word_to_index[word]] for word in filtered_words])\n",
    "\n",
    "# Bag of Words (Count Vectorizer)\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform([' '.join(filtered_words)])\n",
    "\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_representation = tfidf_vectorizer.fit_transform([' '.join(filtered_words)])\n",
    "\n",
    "# Displaying Outputs\n",
    "print(\"One-Hot Encoded Representation (Sample):\")\n",
    "print(one_hot_encoded[:5])\n",
    "\n",
    "print(\"\\nBag of Words Representation:\")\n",
    "print(bag_of_words.toarray())\n",
    "\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "print(tfidf_representation.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5674a620-6f3a-4bc5-8275-fff73afb6e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['بريا', 'بايدن', 'صحيفة', 'بوليتيكو', 'عن', 'مسوول', 'اميركي', 'قوله', 'ان', 'اسراييل', 'ابلغت', 'ادارة', 'الرييس', 'الاميركي', 'جو', 'ومنظمات', 'لاجتياحها', 'اغاثية', 'انها', 'وضعت', 'خطة', 'لاجلاء', 'السكان', 'من', 'مدينة', 'رفح', 'جنوب', 'قطاع', 'غزة', 'استعدادا', 'نقلت']\n"
     ]
    }
   ],
   "source": [
    "word_vectors = skipgram_model.wv  # Assuming skipgram_model is your trained Word2Vec model\n",
    "print(\"Vocabulary:\", list(word_vectors.key_to_index.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "930b7bf8-1805-4a52-b0a7-b6ae199ea681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'بايدن':\n",
      "[-8.6323451e-03  3.6718368e-03  5.1901834e-03  5.7580625e-03\n",
      "  7.5008231e-03 -6.1662658e-03  1.1016206e-03  6.0598492e-03\n",
      " -2.8371220e-03 -6.1826496e-03 -4.0837994e-04 -8.3966348e-03\n",
      " -5.6084786e-03  7.1230996e-03  3.3571371e-03  7.2341803e-03\n",
      "  6.8041617e-03  7.5379545e-03 -3.8080460e-03 -5.8632815e-04\n",
      "  2.3672825e-03 -4.5010955e-03  8.3909081e-03 -9.8476149e-03\n",
      "  6.7710797e-03  2.9317706e-03 -4.9304045e-03  4.3923701e-03\n",
      " -1.7514927e-03  6.7024254e-03  9.9712852e-03 -4.3848888e-03\n",
      " -5.9859012e-04 -5.7103471e-03  3.8565316e-03  2.7900871e-03\n",
      "  6.9128233e-03  6.1296024e-03  9.5453998e-03  9.2886752e-03\n",
      "  7.9115145e-03 -7.0138364e-03 -9.1757756e-03 -3.2358404e-04\n",
      " -3.0940019e-03  7.8888880e-03  5.9102392e-03 -1.5555222e-03\n",
      "  1.5305692e-03  1.7966764e-03  7.8364350e-03 -9.5329909e-03\n",
      " -2.0389009e-04  3.4679649e-03 -9.5300050e-04  8.3959522e-03\n",
      "  9.0397270e-03  6.5355017e-03 -7.2237593e-04  7.7135311e-03\n",
      " -8.5447505e-03  3.2149835e-03 -4.6170740e-03 -5.0874171e-03\n",
      "  3.5790587e-03  5.3946157e-03  7.7646454e-03 -5.7652784e-03\n",
      "  7.4484297e-03  6.6331294e-03 -3.7093691e-03 -8.7470068e-03\n",
      "  5.4304791e-03  6.5132366e-03 -7.7892398e-04 -6.7168032e-03\n",
      " -7.0741209e-03 -2.4884793e-03  5.1487833e-03 -3.6760732e-03\n",
      " -9.4079422e-03  3.8172482e-03  4.8805224e-03 -6.4491695e-03\n",
      "  1.1928327e-03 -2.0485867e-03  3.1743300e-05 -9.8914923e-03\n",
      "  2.6804907e-03 -4.7519105e-03  1.0928311e-03 -1.5565050e-03\n",
      "  2.2164206e-03 -7.8603672e-03 -2.7086507e-03  2.6664778e-03\n",
      "  5.3618746e-03 -2.3929768e-03 -9.5522255e-03  4.5253485e-03]\n"
     ]
    }
   ],
   "source": [
    "# Check if target word exists in the vocabulary\n",
    "target_word = 'بايدن'\n",
    "if target_word in word_vectors:\n",
    "    # Access the word vector\n",
    "    print(\"Vector representation of '{}':\".format(target_word))\n",
    "    print(word_vectors[target_word])\n",
    "else:\n",
    "    print(\"Word '{}' is not present in the vocabulary.\".format(target_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e6068ab-792a-4b5b-a6b8-c97fc9f333c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.3.0-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.3.0-cp311-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-2.3.0\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchtext) (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchtext) (1.24.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch>=2.3.0->torchtext) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
      "Downloading torchtext-0.18.0-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchtext\n",
      "Successfully installed torchtext-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b6c5c-1d76-4bda-91a8-383c5e5f155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      ".vector_cache/glove.840B.300d.zip:   6%|▎    | 126M/2.18G [01:37<35:53, 952kB/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "# Load the pre-trained GloVe model\n",
    "glove = vocab.GloVe(name='840B', dim=300)\n",
    "\n",
    "# Load the pre-trained FastText model\n",
    "fasttext = vocab.FastText(language='ar')\n",
    "\n",
    "# Define word pairs for similarity comparison\n",
    "word_pairs = [('رفح', 'قطاع'), ('منظمات', 'إغاثة'), ('إسرائيل', 'أميركي')]\n",
    "\n",
    "# Compute similarity for each pair of words using GloVe\n",
    "print(\"Similarity between word pairs using GloVe:\")\n",
    "for pair in word_pairs:\n",
    "    vec1, vec2 = glove[pair[0]], glove[pair[1]]\n",
    "    similarity = torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}': {similarity:.3f}\")\n",
    "\n",
    "# Compute similarity for each pair of words using FastText\n",
    "print(\"\\nSimilarity between word pairs using FastText:\")\n",
    "for pair in word_pairs:\n",
    "    vec1, vec2 = fasttext[pair[0]], fasttext[pair[1]]\n",
    "    similarity = torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}': {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf7210-15e2-4ed6-8074-938b28879778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained GloVe model\n",
    "glove = vocab.GloVe(name='840B', dim=300)\n",
    "\n",
    "# Load the pre-trained FastText model\n",
    "fasttext = vocab.FastText(language='ar')\n",
    "\n",
    "# Collect word vectors and corresponding labels\n",
    "word_vectors = []\n",
    "labels = []\n",
    "\n",
    "# Choose a subset of words for plotting (e.g., 100 words)\n",
    "word_subset = list(glove.stoi.keys())[:100]\n",
    "\n",
    "for word in word_subset:\n",
    "    if word in glove.stoi and word in fasttext.stoi:\n",
    "        # Collect word vectors\n",
    "        glove_vector = glove[word]\n",
    "        fasttext_vector = fasttext[word]\n",
    "        \n",
    "        # Append word vectors and labels\n",
    "        word_vectors.append(glove_vector)\n",
    "        labels.append(word)\n",
    "\n",
    "# Convert word vectors to numpy array\n",
    "word_vectors = torch.stack(word_vectors).numpy()\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], marker='o')\n",
    "\n",
    "# Add labels to the plot\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "\n",
    "plt.title('t-SNE Visualization of GloVe and FastText Embeddings')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
